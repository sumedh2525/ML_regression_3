{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ac8833-efd2-47b4-81dd-5c61c07f3fac",
   "metadata": {},
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f0e616-93b4-4258-b35c-21bd49c6e773",
   "metadata": {},
   "source": [
    "Ridge Regression, also known as Tikhonov regularization or L2 regularization, is a linear regression technique used to address multicollinearity and overfitting in regression models. It achieves this by adding a penalty term to the ordinary least squares (OLS) regression loss function. The penalty term is proportional to the sum of squared coefficients, and its purpose is to restrict the magnitude of the coefficients, especially for variables that are highly correlated.\n",
    "\n",
    "Here's how Ridge Regression differs from Ordinary Least Squares (OLS) Regression:\n",
    "\n",
    "Loss Function:\n",
    "\n",
    "OLS Regression: In OLS regression, the goal is to minimize the sum of squared residuals (differences between predicted and actual values).\n",
    "Ridge Regression: In Ridge regression, the loss function combines the sum of squared residuals with a penalty term proportional to the sum of squared coefficients. The goal is to minimize the sum of squared residuals while also constraining the coefficient magnitudes.\n",
    "Penalty Term:\n",
    "\n",
    "OLS Regression: OLS regression does not include any penalty term in its loss function. It seeks to fit the data by adjusting the coefficients to minimize the sum of squared residuals only.\n",
    "Ridge Regression: Ridge regression includes a penalty term that encourages smaller coefficients by adding a regularization parameter (Î±) times the sum of squared coefficients to the loss function.\n",
    "Coefficient Shrinking:\n",
    "\n",
    "OLS Regression: OLS regression coefficients can become very large, especially when multicollinearity is present. This can lead to overfitting, especially when the number of observations is small relative to the number of predictors.\n",
    "Ridge Regression: Ridge regression forces coefficients to be smaller by penalizing large values. The larger the coefficients, the larger the penalty. This helps prevent overfitting and can mitigate the impact of multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce335ce-84b2-45dc-9e2a-ab74ff10a146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2325db3-c673-4c99-b815-9929277dc2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b362150-7389-4384-b80f-5377d7cab293",
   "metadata": {},
   "source": [
    "Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f05b0-b273-45a9-98e9-0058798af57d",
   "metadata": {},
   "source": [
    "Ridge Regression is a technique based on linear regression, and many of its assumptions are similar to those of ordinary least squares (OLS) regression. However, Ridge Regression introduces a regularization term that can influence the interpretation of the assumptions. Here are the key assumptions of Ridge Regression:\n",
    "\n",
    "Linearity: The relationship between the independent variables and the dependent variable is assumed to be linear. This means that changes in the independent variables are associated with a constant change in the dependent variable.\n",
    "\n",
    "Independence: The errors (residuals) should be independent of each other. In the context of Ridge Regression, this assumption still holds, as the regularization term is added to the loss function to control the coefficients' magnitudes, not their independence.\n",
    "\n",
    "Homoscedasticity: The errors should have constant variance across all levels of the independent variables. Ridge Regression does not inherently address this assumption, so it's important to check for homoscedasticity in the residuals as part of the model evaluation process.\n",
    "\n",
    "Normality of Errors: The errors are assumed to follow a normal distribution. While Ridge Regression doesn't directly affect this assumption, it's still important to check the normality of residuals to ensure the model's validity.\n",
    "\n",
    "No Perfect Multicollinearity: The independent variables should not be perfectly correlated. Ridge Regression is often used to address multicollinearity, but extremely high multicollinearity can still be problematic.\n",
    "\n",
    "No Endogeneity: The errors should not be correlated with the independent variables. Ridge Regression doesn't address this assumption explicitly, so it's important to consider endogeneity issues separately.\n",
    "\n",
    "No Overfitting: Ridge Regression is used to prevent overfitting, but it assumes that the model has not been overfitted to the training data. It helps control overfitting by regularizing the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81fa74-9499-4d10-9df0-a71c2c759844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11205188-e3aa-4240-bf72-67ace3163ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47c112da-3655-4fce-b9d7-cca76fb7610f",
   "metadata": {},
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7586802c-82d0-4bbc-b54d-9b7a273973f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha using Cross-Validation: 0.1\n",
      "Best alpha using Grid Search: 0.1\n",
      "MSE using Cross-Validation: 2856.4868876706537\n",
      "MSE using Grid Search: 2856.4868876706537\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load a sample dataset (e.g., diabetes dataset)\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform Ridge Regression with Cross-Validation\n",
    "alphas = np.logspace(-6, 6, 13)\n",
    "cv_scores = []\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    scores = cross_val_score(ridge, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_scores.append(np.mean(np.abs(scores)))\n",
    "\n",
    "best_alpha_cv = alphas[np.argmin(cv_scores)]\n",
    "print(\"Best alpha using Cross-Validation:\", best_alpha_cv)\n",
    "\n",
    "# Perform Ridge Regression with Grid Search\n",
    "param_grid = {'alpha': alphas}\n",
    "ridge_grid = GridSearchCV(Ridge(), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "best_alpha_grid = ridge_grid.best_params_['alpha']\n",
    "print(\"Best alpha using Grid Search:\", best_alpha_grid)\n",
    "\n",
    "# Train Ridge Regression with the best alpha on the full training data\n",
    "ridge_cv = Ridge(alpha=best_alpha_cv)\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "ridge_grid = Ridge(alpha=best_alpha_grid)\n",
    "ridge_grid.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred_cv = ridge_cv.predict(X_test)\n",
    "y_pred_grid = ridge_grid.predict(X_test)\n",
    "\n",
    "mse_cv = mean_squared_error(y_test, y_pred_cv)\n",
    "mse_grid = mean_squared_error(y_test, y_pred_grid)\n",
    "\n",
    "print(\"MSE using Cross-Validation:\", mse_cv)\n",
    "print(\"MSE using Grid Search:\", mse_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b149cb-a9e3-4f58-b27f-b9eb4ad27fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21686d9-6e8b-46d2-93ce-b8f2c4ff9aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8393067a-a4c9-457f-8f97-56472fe02fb3",
   "metadata": {},
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c68092-5136-4a35-bc13-0013a4c895a1",
   "metadata": {},
   "source": [
    "Ridge Regression, a regularization technique, is primarily used for addressing multicollinearity and overfitting in linear regression models. While it can indirectly aid in feature selection by shrinking the coefficients of less important features towards zero, it's not explicitly designed for feature selection. Ridge Regression does not eliminate features; it retains all features but adjusts their coefficients.\n",
    "\n",
    "However, there is a related technique called Lasso Regression (Least Absolute Shrinkage and Selection Operator) that is often used for feature selection. Lasso Regression adds a penalty term to the linear regression objective function that encourages some coefficients to become exactly zero. This leads to automatic feature selection by eliminating less important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38868fe-3e69-43f5-b1d8-dcd67f672baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3945bc-6059-42fc-9bd9-ec7f7931a487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b61bce6-d567-4413-ba73-600583f6c682",
   "metadata": {},
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023b37f-07f0-4018-844c-7ba04e9527c2",
   "metadata": {},
   "source": [
    "Ridge Regression is particularly useful in the presence of multicollinearity, which is a situation where two or more independent variables in a linear regression model are highly correlated. In the presence of multicollinearity, the standard linear regression model can become unstable and produce unreliable coefficient estimates. This is because multicollinearity makes it difficult to distinguish the individual effects of correlated variables on the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410cba02-eaf4-410a-bb34-6bca3e35022d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04326c50-2597-4083-a380-aebac5b9f2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed475d73-d400-4677-96f6-043574326527",
   "metadata": {},
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67e6153-95c1-4168-ab02-78311a5d2a1a",
   "metadata": {},
   "source": [
    "No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299c16a-0bcd-4419-8a69-c57ac1b223ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf48b757-714a-4ab1-888d-594a44984945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "602e899c-0873-497a-914d-a538fd91d270",
   "metadata": {},
   "source": [
    "Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f0e43-9dda-479e-a1ad-86dfcf2da392",
   "metadata": {},
   "source": [
    "Magnitude: The magnitude of the coefficient indicates the strength of the relationship between the corresponding independent variable and the dependent variable. Larger magnitudes suggest stronger influences on the dependent variable.\n",
    "\n",
    "Sign: The sign of the coefficient (positive or negative) indicates the direction of the relationship. A positive coefficient suggests that an increase in the independent variable is associated with an increase in the dependent variable, while a negative coefficient suggests the opposite.\n",
    "\n",
    "Relative Importance: Comparing the magnitudes of coefficients within the same model can give you an idea of the relative importance of different variables. However, be cautious when comparing coefficients across different models or datasets, as the scale of the variables can affect the interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e606014-e5e4-49e0-9998-cb88c5681965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3e6ed8-b4a1-46cf-816c-3d7d5aa2327a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6989788-afb4-44f4-b8b2-953a6440dbc7",
   "metadata": {},
   "source": [
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d141e8-fc2c-48bd-825a-6a4cf71143e3",
   "metadata": {},
   "source": [
    "Temporal Structure: Time-series data have a temporal structure, meaning that observations are ordered chronologically. This structure must be preserved when splitting the data into training and testing sets to avoid data leakage.\n",
    "\n",
    "Stationarity: Time-series data often exhibit trends, seasonality, and other patterns that can affect the model's performance. It's essential to preprocess the data to ensure stationarity, which can involve differencing, detrending, or using other methods to make the data more suitable for modeling.\n",
    "\n",
    "Lagged Variables: Time-series models often incorporate lagged versions of the dependent and/or independent variables. These lagged variables capture the temporal dependencies present in the data and are important for accurate modeling.\n",
    "\n",
    "Feature Engineering: In addition to lagged variables, you may need to engineer other relevant features that capture the underlying patterns or relationships in the time-series data.\n",
    "\n",
    "Regularization Parameter Selection: The choice of the regularization parameter (lambda) in Ridge Regression remains important. Cross-validation or other techniques can help determine the optimal lambda for your time-series model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a80792f-6ab5-4dfd-8e6d-37189cc2afc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9146e33-6211-4f79-8b26-f8b26fdcb510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
